## Hyperparameter 
Parameters that affect the performance and structure of an ML algorithm.
<img src="https://i.imgur.com/7tCGXHy.png" height="300" />

For example:
__Linear Regression__ : __Learning Rate__ (Step Size), Batch Size (sample batch size), Epochs (times algo process entire data ex: 100,1000)
__K Nearest Neighbors__: n_neighbors, metric
__Decision Trees__: max_depth, min_samples_leaf, class_weight, criterion
__Random Forest__: n_estimators, max_samples
__Logistic Regression__: penalty, C, class_weight, l1_ratio

* Grid Search - Brute force/exhaustive tuning
Alternative is
* Random Search - randomly tuning the parameters
* Bayesian Search - Keeps track of last hyper-tuning evaluations and build probabilistic model

## Steps to choose hyperparameter
1. Choose tunable hyperparameter, then 
2. choose a range of values SageMaker can use on that tunable hyperparameter. Then,
3. choose the objective metric we want SageMaker to watch as it adjusts the tunable hyperparameter.

**For Example**
* Decision trees: Depth, max number of splits, impurity measure (entropy, gini etc.)
* K Nearest Neighbors: K (number of neighbors)
* Ensemble Bagging model: Number of individual models

### Learning Rate [0.0 - 0.1] | Gradient decent
too slow - slow to find local minimum and global minimum
too fast - might get stuck in local minimum
<img src="https://i.imgur.com/cTyO5aF.png" height="300" />

### Batch Size | number of samples
too small - slow to find local minimum and global minimum
too large - might get stuck in local minimum


